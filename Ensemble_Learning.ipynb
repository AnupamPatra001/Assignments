{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Question 1: What is Ensemble Learning in machine learning? Explain the key idea behind it.**\n",
        "\n",
        "\n",
        "Ensemble Learning is a concept in machine learning where we combine the power of multiple models to make better and more reliable predictions than any single model alone.\n",
        "\n",
        "Think of it like this — if you ask one person for an opinion, you might get a biased answer. But if you ask a group of experts and then take their collective decision, you usually end up with a more accurate result. That's exactly what ensemble learning does — it takes the “wisdom of the crowd” approach.\n",
        "\n",
        "In simple terms, instead of depending on just one model (like one decision tree), we train several models and then combine their outputs. The main idea is that each model may make different kinds of mistakes, and by averaging or voting among them, we can cancel out many of those errors.\n",
        "\n",
        "There are two main types of ensemble methods:\n",
        "\n",
        "Bagging (Bootstrap Aggregating) - It trains multiple models independently on random subsets of the data and then combines their results. Example: Random Forest.\n",
        "\n",
        "Boosting - It trains models one after another, where each new model focuses on the mistakes of the previous one. Example: AdaBoost, XGBoost.\n",
        "\n",
        "**Question 2: What is the difference between Bagging and Boosting?**\n",
        "\n",
        "1. Bagging (Bootstrap Aggregating)\n",
        "\n",
        "Bagging trains many models independently on random subsets of the data. Each model learns on a slightly different sample, and in the end, their predictions are combined — usually by taking a majority vote (for classification) or an average (for regression).\n",
        "\n",
        "The main goal of bagging is to reduce variance — that means it makes the model more stable and less likely to overfit.\n",
        "A classic example of bagging is the Random Forest, which builds many decision trees and combines their results.\n",
        "\n",
        " Think of bagging like asking multiple students to solve the same question separately and then taking the average of their answers — random errors get canceled out, and the final result is more reliable.\n",
        "\n",
        "2. Boosting\n",
        "\n",
        "Boosting, on the other hand, works sequentially — it builds models one after another. Each new model tries to fix the mistakes made by the previous ones.\n",
        "\n",
        "The idea is that later models pay more attention to the data points that were misclassified earlier. Over time, the combined model becomes very powerful.\n",
        "Examples include AdaBoost, Gradient Boosting, and XGBoost.\n",
        "\n",
        " Think of boosting like a teacher giving extra attention to the students who got the answers wrong in the last test — gradually, the whole class improves.\n",
        "\n",
        "**Question 3: What is bootstrap sampling and what role does it play in Bagging methods like Random Forest?**\n",
        "\n",
        "Imagine you have a dataset with 100 rows. Instead of training one model on the full dataset, bootstrap sampling means we randomly pick samples with replacement from those 100 rows to create multiple smaller datasets.\n",
        "Because it’s with replacement, the same row can appear more than once in a sample, and some rows might not appear at all.\n",
        "\n",
        "So, for example:\n",
        "\n",
        "The first model might get data rows 1, 3, 3, 7, 9, 12...\n",
        "\n",
        "The second model might get 2, 5, 8, 8, 10, 15... and so on.\n",
        "\n",
        "Each model (say, each Decision Tree) learns on its own random dataset — a slightly different view of the same problem.\n",
        "\n",
        "Now, what's the role of this in Bagging and Random Forest?\n",
        "\n",
        "The whole point of bootstrap sampling is to introduce diversity among the models. If every model saw the exact same data, they’d all make similar predictions and the ensemble wouldn’t gain much. By training on varied subsets, each tree learns different patterns and makes different mistakes. When their predictions are combined (for example, by taking a majority vote), the errors tend to cancel out, leading to a stronger and more stable overall model.\n",
        "\n",
        "In short:\n",
        "\n",
        "Bootstrap sampling = random sampling with replacement to create different training sets.\n",
        "\n",
        "Its role = to make each model in the ensemble slightly unique, which helps reduce overfitting and improve accuracy.\n",
        "\n",
        "**Question 4: What are Out-of-Bag (OOB) samples and how is OOB score used to evaluate ensemble models?**\n",
        "\n",
        "When we use bootstrap sampling (like in Random Forests), not every data point gets selected in each sample — because we pick with replacement. On average, about two-thirds of the data is used to train each model, and the remaining one-third is left out.\n",
        "\n",
        "These leftover data points that weren't chosen for a particular tree are called Out-of-Bag (OOB) samples.\n",
        "\n",
        "Now, here’s the smart part — instead of setting aside a separate test dataset, we can use these OOB samples to estimate how well our model is performing.\n",
        "\n",
        "Here’s how it works step by step:\n",
        "\n",
        "Each tree in the Random Forest is trained on its own bootstrap sample.\n",
        "\n",
        "For that tree, the data points that were not included (the OOB samples) can be used to test how well that tree predicts unseen data.\n",
        "\n",
        "This process repeats for every tree, and finally, we average all those predictions to calculate what's called the OOB score — which is basically an internal accuracy estimate of the model.\n",
        "\n",
        "So, in simple terms:\n",
        "\n",
        "OOB samples are the data points that a tree didn't see during training.\n",
        "\n",
        "OOB score measures how accurately the model predicts those unseen samples.\n",
        "\n",
        "The great thing about this is that it gives you a built-in validation check — meaning you can get an unbiased estimate of model performance without needing a separate validation or test set.\n",
        "\n",
        "**Question 5: Compare feature importance analysis in a single Decision Tree vs a Random Forest.**\n",
        "\n",
        "1. In a Single Decision Tree\n",
        "\n",
        "In a single Decision Tree, feature importance is calculated based on how much each feature reduces impurity (like Gini impurity or entropy) every time it's used to split the data.\n",
        "So, the more a feature helps the tree make “pure” groups — meaning groups that mostly belong to one class — the more important it's considered.\n",
        "\n",
        "However, since it's just one tree, this method can be unstable.\n",
        "If you slightly change the data, the tree structure might change a lot, and so will the feature importances. That's why single-tree feature importance can sometimes be biased or inconsistent.\n",
        "\n",
        " Example:\n",
        "If one feature happens to dominate the splits early in the tree, it might get a very high importance score, even if it's not always the most useful feature across the dataset.\n",
        "\n",
        "2. In a Random Forest\n",
        "\n",
        "A Random Forest, on the other hand, builds many trees, each trained on different random subsets of data and features.\n",
        "For feature importance, it looks at how much each feature reduces impurity across all the trees and then averages these contributions.\n",
        "\n",
        "This makes the importance values more stable, balanced, and reliable, because they represent the collective judgment of many models — not just one.\n",
        "\n",
        " Example:\n",
        "Even if one feature doesn't show up in a few trees, its overall importance across 100 or 200 trees gives a fair estimate of how useful it really is."
      ],
      "metadata": {
        "id": "DH3haGYxaGSf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hx9qE6S4aDos",
        "outputId": "9881188c-c11b-4a71-e335-4f6835d12cba"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                 Feature  Importance\n",
            "23            worst area    0.153892\n",
            "27  worst concave points    0.144663\n",
            "7    mean concave points    0.106210\n",
            "20          worst radius    0.077987\n",
            "6         mean concavity    0.068001\n"
          ]
        }
      ],
      "source": [
        "# Question 6: Write a Python program to:\n",
        "# ● Load the Breast Cancer dataset using\n",
        "# sklearn.datasets.load_breast_cancer()\n",
        "# ● Train a Random Forest Classifier\n",
        "# ● Print the top 5 most important features based on feature importance scores.\n",
        "# (Include your Python code and output in the code box below.)\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "model = RandomForestClassifier(random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "importances = model.feature_importances_\n",
        "feature_names = data.feature_names\n",
        "\n",
        "importance_df = pd.DataFrame({\n",
        "    'Feature': feature_names,\n",
        "    'Importance': importances\n",
        "})\n",
        "\n",
        "top5 = importance_df.sort_values(by='Importance', ascending=False).head(5)\n",
        "print(top5)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "dt = DecisionTreeClassifier(random_state=42)\n",
        "dt.fit(X_train, y_train)\n",
        "dt_pred = dt.predict(X_test)\n",
        "dt_acc = accuracy_score(y_test, dt_pred)\n",
        "\n",
        "bag = BaggingClassifier(estimator=DecisionTreeClassifier(), n_estimators=50, random_state=42)\n",
        "bag.fit(X_train, y_train)\n",
        "bag_pred = bag.predict(X_test)\n",
        "bag_acc = accuracy_score(y_test, bag_pred)\n",
        "\n",
        "print(\"Single Decision Tree Accuracy:\", dt_acc)\n",
        "print(\"Bagging Classifier Accuracy:\", bag_acc)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8ZSSBhHQcdjc",
        "outputId": "47673c70-a4a7-42b7-803a-329ccb9d0333"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Single Decision Tree Accuracy: 1.0\n",
            "Bagging Classifier Accuracy: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Question 8: Write a Python program to:\n",
        "# ● Train a Random Forest Classifier\n",
        "# ● Tune hyperparameters max_depth and n_estimators using GridSearchCV\n",
        "# ● Print the best parameters and final accuracy\n",
        "# (Include your Python code and output in the code box below.)\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "param_grid = {\n",
        "    'max_depth': [2, 3, 4, 5, None],\n",
        "    'n_estimators': [10, 50, 100, 150]\n",
        "}\n",
        "\n",
        "grid_search = GridSearchCV(RandomForestClassifier(random_state=42), param_grid, cv=5)\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "best_model = grid_search.best_estimator_\n",
        "y_pred = best_model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "print(\"Best Parameters:\", grid_search.best_params_)\n",
        "print(f\"Final Model Accuracy: {accuracy:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rbYMP2vCcn89",
        "outputId": "df2ea5e7-49ce-4739-caa5-113126a12226"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters: {'max_depth': 3, 'n_estimators': 50}\n",
            "Final Model Accuracy: 1.00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Question 9: Write a Python program to:\n",
        "# ● Train a Bagging Regressor and a Random Forest Regressor on the California\n",
        "# Housing dataset\n",
        "# ● Compare their Mean Squared Errors (MSE)\n",
        "# (Include your Python code and output in the code box below.)\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import BaggingRegressor, RandomForestRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "data = fetch_california_housing()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "base_estimator = DecisionTreeRegressor(random_state=42)\n",
        "\n",
        "bagging_regressor = BaggingRegressor(estimator=base_estimator, n_estimators=100, random_state=42)\n",
        "bagging_regressor.fit(X_train, y_train)\n",
        "\n",
        "rf_regressor = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "rf_regressor.fit(X_train, y_train)\n",
        "\n",
        "y_pred_bagging = bagging_regressor.predict(X_test)\n",
        "y_pred_rf = rf_regressor.predict(X_test)\n",
        "\n",
        "mse_bagging = mean_squared_error(y_test, y_pred_bagging)\n",
        "mse_rf = mean_squared_error(y_test, y_pred_rf)\n",
        "\n",
        "print(\"Mean Squared Error (Bagging Regressor):\", round(mse_bagging, 4))\n",
        "print(\"Mean Squared Error (Random Forest Regressor):\", round(mse_rf, 4))\n",
        "\n",
        "if mse_rf < mse_bagging:\n",
        "    print(\"\\nRandom Forest Regressor performs better (lower MSE).\")\n",
        "else:\n",
        "    print(\"\\nBagging Regressor performs better (lower MSE).\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gjHn-sT2dBbP",
        "outputId": "5c468bd0-a4fa-42c5-a705-529e917340d4"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Squared Error (Bagging Regressor): 0.2559\n",
            "Mean Squared Error (Random Forest Regressor): 0.2554\n",
            "\n",
            "Random Forest Regressor performs better (lower MSE).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Question 10: You are working as a data scientist at a financial institution to predict loan\n",
        "# default. You have access to customer demographic and transaction history data.\n",
        "# You decide to use ensemble techniques to increase model performance.\n",
        "# Explain your step-by-step approach to:\n",
        "# ● Choose between Bagging or Boosting\n",
        "# ● Handle overfitting\n",
        "# ● Select base models\n",
        "# ● Evaluate performance using cross-validation\n",
        "# ● Justify how ensemble learning improves decision-making in this real-world\n",
        "# context.\n",
        "# (Include your Python code and output in the code box below.)\n",
        "import pandas as pd\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "\n",
        "X, y = make_classification(n_samples=1000, n_features=10, n_informative=6, n_redundant=2,\n",
        "                           n_classes=2, random_state=42)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "rf = RandomForestClassifier(random_state=42)\n",
        "gb = GradientBoostingClassifier(random_state=42)\n",
        "\n",
        "rf.fit(X_train, y_train)\n",
        "gb.fit(X_train, y_train)\n",
        "\n",
        "rf_pred = rf.predict(X_test)\n",
        "gb_pred = gb.predict(X_test)\n",
        "\n",
        "rf_acc = accuracy_score(y_test, rf_pred)\n",
        "gb_acc = accuracy_score(y_test, gb_pred)\n",
        "\n",
        "params = {'max_depth': [3, 5, 7], 'n_estimators': [50, 100, 150]}\n",
        "grid = GridSearchCV(RandomForestClassifier(random_state=42), params, cv=5, scoring='accuracy')\n",
        "grid.fit(X_train, y_train)\n",
        "\n",
        "best_model = grid.best_estimator_\n",
        "best_pred = best_model.predict(X_test)\n",
        "best_acc = accuracy_score(y_test, best_pred)\n",
        "\n",
        "print(\"Random Forest Accuracy:\", rf_acc)\n",
        "print(\"Gradient Boosting Accuracy:\", gb_acc)\n",
        "print(\"Best Random Forest Parameters:\", grid.best_params_)\n",
        "print(\"Best Random Forest Accuracy:\", best_acc)\n",
        "print(\"\\nClassification Report:\\n\", classification_report(y_test, best_pred))\n",
        "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, best_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pqzrms8bdRoc",
        "outputId": "99dc56e7-2d86-4075-fee2-e70aafffda12"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random Forest Accuracy: 0.855\n",
            "Gradient Boosting Accuracy: 0.85\n",
            "Best Random Forest Parameters: {'max_depth': 7, 'n_estimators': 150}\n",
            "Best Random Forest Accuracy: 0.835\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.93      0.77      0.84       113\n",
            "           1       0.75      0.92      0.83        87\n",
            "\n",
            "    accuracy                           0.83       200\n",
            "   macro avg       0.84      0.84      0.83       200\n",
            "weighted avg       0.85      0.83      0.84       200\n",
            "\n",
            "Confusion Matrix:\n",
            " [[87 26]\n",
            " [ 7 80]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QVDKzmiFducY"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}