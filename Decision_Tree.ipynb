{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Question 1: What is a Decision Tree, and how does it work in the context of\n",
        "classification?**\n",
        "\n",
        "A Decision Tree is a type of machine learning algorithm that helps a computer make decisions based on data. It’s mainly used for classification, which means it helps decide which category something belongs to.\n",
        "\n",
        "You can imagine a decision tree as a series of questions that lead to an answer. For example, if you were trying to decide whether someone will buy a computer, you might ask questions like:\n",
        "\n",
        "Is the person older than 30?\n",
        "\n",
        "Do they have a high income?\n",
        "\n",
        "Are they a student?\n",
        "\n",
        "Each question splits the data into smaller groups. At the top of the tree, you start with all the data, and as you move down, you keep dividing it based on the answers to these questions. The process continues until each group at the bottom (called a leaf node) represents a clear decision or class — for example, “Yes, they will buy a computer” or “No, they won't.”\n",
        "\n",
        "The algorithm decides which question to ask at each step by finding the feature that best separates the data. It does this using mathematical measures like Information Gain or Gini Index. These measures help identify which feature gives the most meaningful split.\n",
        "\n",
        "In short, a decision tree works by breaking a complex decision into smaller, simpler ones. It's easy to understand and interpret, but one drawback is that it can sometimes make overly specific decisions if not properly controlled — a problem known as overfitting.\n",
        "\n",
        "**Question 2: Explain the concepts of Gini Impurity and Entropy as impurity measures. How do they impact the splits in a Decision Tree?**\n",
        "\n",
        "When a decision tree decides where to split the data, it needs a way to measure how “mixed” or “uncertain” a group of examples is. Two common ways to measure this are Gini Impurity and Entropy. Both tell the tree how pure or impure a particular node is — in other words, how similar or different the items in that group are.\n",
        "\n",
        "Gini Impurity tells us how often we would get something wrong if we randomly labeled an item from that group.\n",
        "\n",
        "If all items belong to one class (for example, all “Yes” or all “No”), the impurity is 0 — meaning it's perfectly pure.\n",
        "\n",
        "If the group is half “Yes” and half “No,” then the impurity is higher, because there's more confusion or mix.\n",
        "The decision tree always tries to split the data so that each side becomes as pure as possible — that means lower Gini Impurity.\n",
        "\n",
        "Entropy works in a similar way, but it measures the amount of disorder or uncertainty in the data.\n",
        "\n",
        "If a group contains only one type of class, the entropy is 0, meaning it’s perfectly ordered.\n",
        "\n",
        "If the classes are evenly mixed, the entropy is higher because the group is more uncertain.\n",
        "The goal of the tree is to make splits that reduce entropy as much as possible, which is called gaining information.\n",
        "\n",
        "In simple terms, both Gini Impurity and Entropy help the decision tree figure out the best question to ask at each step. The algorithm picks the feature that makes the data more organized — so that the smaller groups it creates are as pure and easy to classify as possible.\n",
        "\n",
        "**Question 3: What is the difference between Pre-Pruning and Post-Pruning in Decision Trees? Give one practical advantage of using each.**\n",
        "\n",
        "When a decision tree is built, it can sometimes become too detailed and start learning noise from the data instead of actual patterns — this problem is called overfitting. To prevent that, we use a process called pruning, which means cutting down parts of the tree that don't add much value.\n",
        "\n",
        "There are two main ways to prune a decision tree: Pre-Pruning and Post-Pruning.\n",
        "\n",
        "Pre-Pruning\n",
        "\n",
        "Pre-Pruning (also known as early stopping) means stopping the tree from growing too much right from the start.\n",
        "While the tree is being built, the algorithm checks certain conditions — like the maximum depth, minimum number of samples in a node, or minimum information gain — and stops splitting when those conditions are met.\n",
        "\n",
        "Example:\n",
        "If we tell the algorithm that the tree can have a maximum depth of 5, it will stop creating new branches after level 5, even if it could go deeper.\n",
        "\n",
        "Practical advantage:\n",
        "Pre-pruning saves both time and computing resources, since it prevents unnecessary growth of the tree. It also helps avoid overfitting early on.\n",
        "\n",
        "Post-Pruning\n",
        "\n",
        "Post-Pruning means letting the tree grow fully first, and then removing the branches that don't improve accuracy.\n",
        "After the complete tree is built, the algorithm tests each branch on a validation dataset and cuts off the parts that don’t contribute to better predictions.\n",
        "\n",
        "Example:\n",
        "If a certain branch only improves the accuracy by a tiny amount on the training data but not on new data, that branch is pruned away.\n",
        "\n",
        "Practical advantage:\n",
        "Post-pruning usually gives better accuracy because the model first explores all possible splits and then removes only the unnecessary ones, leading to a well-balanced tree.\n",
        "\n",
        "**Question 4: What is Information Gain in Decision Trees, and why is it important for choosing the best split?**\n",
        "\n",
        "Information Gain is a concept used in decision trees to decide which feature should be used to split the data at each step. It tells us how much \"information\" or \"certainty\" a feature gives us about the target class when we use it to divide the data.\n",
        "\n",
        "When a decision tree tries to make a split, its main goal is to make the resulting groups as pure as possible — meaning that each group should mostly contain data from one class. To measure this, the algorithm looks at the entropy (the amount of disorder or uncertainty) before and after the split.\n",
        "\n",
        "Information Gain is simply the reduction in entropy after making a split.\n",
        "If splitting the data on a particular feature reduces a lot of uncertainty, that feature gives high information gain — and it's considered a good choice for splitting.\n",
        "\n",
        "In simple words:\n",
        "Information Gain tells the decision tree how much a particular question helps to separate the data clearly.\n",
        "\n",
        "A feature with high Information Gain makes the groups more organized and easier to classify.\n",
        "\n",
        "A feature with low Information Gain doesn't help much in making the groups clearer.\n",
        "\n",
        "Example:\n",
        "If you are predicting whether someone will buy a product, and splitting the data by \"Income Level\" creates two groups where one mostly buys and the other mostly doesn't, that feature gives high Information Gain. On the other hand, if splitting by \"Hair Color\" doesn't change the distribution much, it gives low Information Gain.\n",
        "\n",
        "**Question 5: What are some common real-world applications of Decision Trees, and what are their main advantages and limitations?**\n",
        "\n",
        "\n",
        "Common Real-World Applications\n",
        "\n",
        "Medical Diagnosis\n",
        "Decision trees can help doctors predict whether a patient has a certain disease based on symptoms, test results, or medical history.\n",
        "Example: Predicting if a tumor is benign or malignant.\n",
        "\n",
        "Finance and Banking\n",
        "Banks use decision trees to decide whether to approve a loan or credit card application by checking factors like income, age, and credit score.\n",
        "Example: Loan approval and credit risk assessment.\n",
        "\n",
        "Marketing and Customer Segmentation\n",
        "Businesses use them to identify which type of customers are most likely to buy a product or respond to a campaign.\n",
        "Example: Predicting customer churn or targeting the right audience.\n",
        "\n",
        "Fraud Detection\n",
        "Decision trees can classify transactions as normal or suspicious based on patterns in past data.\n",
        "Example: Detecting fraudulent credit card transactions.\n",
        "\n",
        "Manufacturing and Quality Control\n",
        "Used to determine the causes of defects in production and to maintain consistent product quality.\n",
        "Example: Predicting machine failure or identifying defective products.\n",
        "\n",
        "Main Advantages\n",
        "\n",
        "Easy to Understand:\n",
        "The flowchart-like structure is simple to interpret, even for people without a technical background.\n",
        "\n",
        "No Need for Data Scaling:\n",
        "Decision trees don't require normalization or standardization of features.\n",
        "\n",
        "Works with Both Types of Data:\n",
        "They handle both categorical and numerical data easily.\n",
        "\n",
        "Captures Non-Linear Relationships:\n",
        "They can model complex patterns without needing mathematical transformations.\n",
        "\n",
        "Main Limitations\n",
        "\n",
        "Overfitting:\n",
        "Decision trees can become too complex and start fitting noise instead of real patterns, especially if not pruned properly.\n",
        "\n",
        "Unstable:\n",
        "A small change in the data can lead to a completely different tree structure.\n",
        "\n",
        "Biased Towards Features with More Levels:\n",
        "Features with many categories may dominate the splits, even if they aren’t the most important.\n",
        "\n",
        "Less Accurate Alone:\n",
        "A single decision tree may not be as accurate as ensemble methods like Random Forest or Gradient Boosted Trees.\n",
        "\n"
      ],
      "metadata": {
        "id": "uSTFepGJTLBF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eaRuN7bCS08n",
        "outputId": "80fe7354-0edb-4a1c-bcbe-a36f79fad8bf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Decision Tree Classifier (Gini) Results\n",
            "--------------------------------------\n",
            "Model Accuracy: 1.00\n",
            "Feature Importances:\n",
            "sepal length (cm): 0.0000\n",
            "sepal width (cm): 0.0167\n",
            "petal length (cm): 0.9061\n",
            "petal width (cm): 0.0772\n"
          ]
        }
      ],
      "source": [
        "# Question 6: Write a Python program to:\n",
        "# ● Load the Iris Dataset\n",
        "# ● Train a Decision Tree Classifier using the Gini criterion\n",
        "# ● Print the model’s accuracy and feature importances\n",
        "# (Include your Python code and output in the code box below.)\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "clf = DecisionTreeClassifier(criterion='gini', random_state=42)\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "print(\"Decision Tree Classifier (Gini) Results\")\n",
        "print(\"--------------------------------------\")\n",
        "print(f\"Model Accuracy: {accuracy:.2f}\")\n",
        "print(\"Feature Importances:\")\n",
        "for feature, importance in zip(iris.feature_names, clf.feature_importances_):\n",
        "    print(f\"{feature}: {importance:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Question 7: Write a Python program to:\n",
        "# ● Load the Iris Dataset\n",
        "# ● Train a Decision Tree Classifier with max_depth=3 and compare its accuracy to\n",
        "# a fully-grown tree.\n",
        "# (Include your Python code and output in the code box below.)\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "tree_limited = DecisionTreeClassifier(max_depth=3, random_state=42)\n",
        "tree_limited.fit(X_train, y_train)\n",
        "y_pred_limited = tree_limited.predict(X_test)\n",
        "accuracy_limited = accuracy_score(y_test, y_pred_limited)\n",
        "\n",
        "tree_full = DecisionTreeClassifier(random_state=42)\n",
        "tree_full.fit(X_train, y_train)\n",
        "y_pred_full = tree_full.predict(X_test)\n",
        "accuracy_full = accuracy_score(y_test, y_pred_full)\n",
        "\n",
        "print(\"Decision Tree Accuracy Comparison\")\n",
        "print(\"--------------------------------\")\n",
        "print(f\"Limited Tree (max_depth=3) Accuracy: {accuracy_limited:.2f}\")\n",
        "print(f\"Fully Grown Tree Accuracy: {accuracy_full:.2f}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EZx0dz9vXCTj",
        "outputId": "f4c803b5-e065-454a-fa56-493d9dbd6d77"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Decision Tree Accuracy Comparison\n",
            "--------------------------------\n",
            "Limited Tree (max_depth=3) Accuracy: 1.00\n",
            "Fully Grown Tree Accuracy: 1.00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Question 8: Write a Python program to:\n",
        "# ● Load the Boston Housing Dataset\n",
        "# ● Train a Decision Tree Regressor\n",
        "# ● Print the Mean Squared Error (MSE) and feature importances\n",
        "# (Include your Python code and output in the code box below.)\n",
        "from sklearn.datasets import fetch_openml\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "boston = fetch_openml(name=\"boston\", version=1, as_frame=True)\n",
        "X = boston.data\n",
        "y = boston.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "regressor = DecisionTreeRegressor(random_state=42)\n",
        "regressor.fit(X_train, y_train)\n",
        "\n",
        "y_pred = regressor.predict(X_test)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "\n",
        "print(\"Decision Tree Regressor Results\")\n",
        "print(\"--------------------------------\")\n",
        "print(f\"Mean Squared Error (MSE): {mse:.2f}\")\n",
        "print(\"Feature Importances:\")\n",
        "for feature, importance in zip(X.columns, regressor.feature_importances_):\n",
        "    print(f\"{feature}: {importance:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nfy5H6t7XNAH",
        "outputId": "730c1a82-6d2e-4723-8b79-3b4006120bb9"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Decision Tree Regressor Results\n",
            "--------------------------------\n",
            "Mean Squared Error (MSE): 10.42\n",
            "Feature Importances:\n",
            "CRIM: 0.0513\n",
            "ZN: 0.0034\n",
            "INDUS: 0.0058\n",
            "CHAS: 0.0000\n",
            "NOX: 0.0271\n",
            "RM: 0.6003\n",
            "AGE: 0.0136\n",
            "DIS: 0.0707\n",
            "RAD: 0.0019\n",
            "TAX: 0.0125\n",
            "PTRATIO: 0.0110\n",
            "B: 0.0090\n",
            "LSTAT: 0.1933\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Question 9: Write a Python program to:\n",
        "# ● Load the Iris Dataset\n",
        "# ● Tune the Decision Tree's max_depth and min_samples_split using\n",
        "# GridSearchCV\n",
        "# ● Print the best parameters and the resulting model accuracy\n",
        "# (Include your Python code and output in the code box below.)\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "param_grid = {\n",
        "    'max_depth': [2, 3, 4, 5, None],\n",
        "    'min_samples_split': [2, 3, 4, 5]\n",
        "}\n",
        "\n",
        "grid_search = GridSearchCV(DecisionTreeClassifier(random_state=42), param_grid, cv=5)\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "best_model = grid_search.best_estimator_\n",
        "y_pred = best_model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "print(\"Decision Tree Hyperparameter Tuning Results\")\n",
        "print(\"-------------------------------------------\")\n",
        "print(\"Best Parameters:\", grid_search.best_params_)\n",
        "print(f\"Model Accuracy: {accuracy:.2f}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FLFO-HXcXW7h",
        "outputId": "35310059-8bc8-4112-8b90-d107384b7bf7"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Decision Tree Hyperparameter Tuning Results\n",
            "-------------------------------------------\n",
            "Best Parameters: {'max_depth': 4, 'min_samples_split': 2}\n",
            "Model Accuracy: 1.00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 10: Imagine you're working as a data scientist for a healthcare company that\n",
        "wants to predict whether a patient has a certain disease. You have a large dataset with\n",
        "mixed data types and some missing values.\n",
        "Explain the step-by-step process you would follow to:\n",
        "● Handle the missing values\n",
        "● Encode the categorical features\n",
        "● Train a Decision Tree model\n",
        "● Tune its hyperparameters\n",
        "● Evaluate its performance\n",
        "And describe what business value this model could provide in the real-world\n",
        "setting.**\n",
        "\n",
        "Step 1: Handling Missing Values\n",
        "First, I'd carefully look through the dataset to see where the missing values are. For numerical columns, I'd fill in the missing values with the mean or median of that column, depending on how the data is distributed. For categorical columns, I'd replace missing values with the most frequent category.\n",
        "If some columns have too many missing values (say, more than 40-50%), I might even consider dropping them if they don't add much value.\n",
        "\n",
        "Step 2: Encoding Categorical Features\n",
        "Next, since decision trees can't understand text directly, I'd convert all categorical data into numerical form.\n",
        "For columns with only a few categories (like “Gender” or “Smoker/Non-Smoker”), I'd use Label Encoding.\n",
        "For columns with many unique categories (like “City” or “Blood Type”), I'd use One-Hot Encoding so that no single value dominates the others.\n",
        "\n",
        "Step 3: Training the Decision Tree Model\n",
        "After cleaning and encoding the data, I'd split the dataset into training and testing sets — typically 80% for training and 20% for testing. Then I'd create a Decision Tree Classifier, train it on the training data, and check how well it predicts on the test data.\n",
        "\n",
        "Step 4: Tuning Hyperparameters\n",
        "To make sure the model doesn't overfit (memorize) or underfit (miss patterns), I'd tune hyperparameters like:\n",
        "\n",
        "max_depth (how deep the tree can go)\n",
        "\n",
        "min_samples_split (minimum samples needed to split a node)\n",
        "\n",
        "criterion (whether to use “gini” or “entropy”)\n",
        "\n",
        "I'd use GridSearchCV to automatically test different combinations of these parameters and find the best one.\n",
        "\n",
        "Step 5: Evaluating the Model\n",
        "Once the best model is ready, I'd evaluate it using metrics like:\n",
        "\n",
        "Accuracy — how many predictions are correct\n",
        "\n",
        "Precision & Recall — to check how well it identifies patients with the disease\n",
        "\n",
        "F1-Score — a balance between precision and recall\n",
        "\n",
        "Confusion Matrix — to visualize correct and incorrect predictions\n",
        "\n",
        "If the dataset is imbalanced (for example, very few patients have the disease), I'd use techniques like SMOTE (Synthetic Minority Oversampling) or use the AUC-ROC score to better understand performance.\n",
        "\n",
        "Business Value in the Real World\n",
        "In a healthcare setting, such a model could be incredibly valuable. It could help doctors quickly identify high-risk patients and prioritize medical attention. It could also assist in early detection, helping hospitals manage resources better and improving patient outcomes.\n",
        "Ultimately, the model wouldn't replace doctors — it would support them by providing data-driven insights, saving time, and potentially saving lives."
      ],
      "metadata": {
        "id": "pEEe-4ASXzLD"
      }
    }
  ]
}